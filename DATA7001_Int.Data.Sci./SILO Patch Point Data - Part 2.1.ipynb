{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Is my data fit for use - Hive\n",
    "\n",
    "The SILO data has already been loaded into Hive and HDFS for you. As you will recall from earlier pracs, once the data is in Hive we can use SQL to ask a variety of different questions.\n",
    "\n",
    "To use Hive, SSH into the data7001 node with your UQ username and type `hive` at the prompt.\n",
    "\n",
    "Let's use the `show tables` command to see the available tables:\n",
    "\n",
    "```\n",
    "hive> show tables;\n",
    "OK\n",
    "mdata\n",
    "silo\n",
    "Time taken: 0.242 seconds, Fetched: 2 row(s)\n",
    "```\n",
    "\n",
    "You'll see that there are two tables ready to be queried, one called `mdata` and one called `silo`.\n",
    "\n",
    "What format are these tables in? We can use the `desc` command to find out:\n",
    "\n",
    "```\n",
    "hive> desc mdata;\n",
    "OK\n",
    "fname                   string                                      \n",
    "latitude                float                                       \n",
    "elevation               float                                       \n",
    "id                      int                                         \n",
    "longitude               float                                       \n",
    "name                    string                                      \n",
    "Time taken: 0.42 seconds, Fetched: 6 row(s)\n",
    "hive> desc silo;\n",
    "OK\n",
    "date1                   int                                         \n",
    "day                     int                                         \n",
    "date2                   string                                      \n",
    "tmax                    float                                       \n",
    "smx                     int                                         \n",
    "tmin                    float                                       \n",
    "smn                     int                                         \n",
    "rain                    float                                       \n",
    "srn                     int                                         \n",
    "evap                    float                                       \n",
    "sev                     int                                         \n",
    "radn                    float                                       \n",
    "ssl                     int                                         \n",
    "vp                      float                                       \n",
    "svp                     int                                         \n",
    "rhmaxt                  float                                       \n",
    "rhmint                  float                                       \n",
    "fa056                   float                                       \n",
    "ascepm                  float                                       \n",
    "mlake                   float                                       \n",
    "mpot                    float                                       \n",
    "mact                    float                                       \n",
    "mwet                    float                                       \n",
    "span                    float                                       \n",
    "ssp                     int                                 \n",
    "evsp                    float                                       \n",
    "ses                     int                                         \n",
    "mslpres                 float                                       \n",
    "sp                      int                                         \n",
    "Time taken: 0.414 seconds, Fetched: 29 row(s)\n",
    "```\n",
    "\n",
    "From the previous part of the prac, we noticed that picking a dataset at random gave us a lot of variability in how many data points were interpolated vs recorded. Let's see if we can find a dataset where most of the Maximum Temperature values are actual recorded values rather than Interpolated. Recall the table from last week the various source codes:\n",
    "\n",
    "| source code | description\n",
    "| ---------- |\n",
    "| 0 | Station data, as supplied by Bureau |\n",
    "| 13 | Deaccumulated using nearby station |\n",
    "| 15 | Deaccumulated using interpolated data |\n",
    "| 23 | Nearby station, data from BoM | \n",
    "| 25 | interpolated daily observations |\n",
    "| 26 | synthetic pan evaporation (only used for Ssp) |\n",
    "| 35 | interpolated from daily observations using anomaly interpolation method for CLIMARC data | \n",
    "| 75 | interpolated long term average | \n",
    "\n",
    "In this case, for each *individual file* we want to count how many entries exist where 'smx' (source of max temperature) is equal to 0. To do this, we would need to read every line of each individual file, check the source column and sum the refrequencies.\n",
    "\n",
    "We could do this with a simple python script, but given how much data we have to parse, a faster solution may be to use Hive to distribute the process across several machines. For a simple GROUP BY query, it also saves us having to write a custom parsing script, since the data is already in Hive ready to be queried.\n",
    "\n",
    "To run this query in Hive, we can use the following to submit a Map Reduce job that counts the frequencies for each file:\n",
    "\n",
    "```\n",
    "hive> select count(*) as cnt, INPUT__FILE__NAME from silo where smx =\n",
    "'0' GROUP BY INPUT__FILE__NAME ORDER BY cnt desc LIMIT 10;\n",
    "\n",
    "46674 hdfs://node1.fabric.zones.eait.uq.edu.au:8020/shared/silo/66062_UQ.dat\n",
    "45552 hdfs://node1.fabric.zones.eait.uq.edu.au:8020/shared/silo/26026_UQ.dat\n",
    "44455 hdfs://node1.fabric.zones.eait.uq.edu.au:8020/shared/silo/44021_UQ.dat\n",
    "44082 hdfs://node1.fabric.zones.eait.uq.edu.au:8020/shared/silo/38003_UQ.dat\n",
    "43968 hdfs://node1.fabric.zones.eait.uq.edu.au:8020/shared/silo/30045_UQ.dat\n",
    "41333 hdfs://node1.fabric.zones.eait.uq.edu.au:8020/shared/silo/39039_UQ.dat\n",
    "40960 hdfs://node1.fabric.zones.eait.uq.edu.au:8020/shared/silo/74128_UQ.dat\n",
    "39988 hdfs://node1.fabric.zones.eait.uq.edu.au:8020/shared/silo/9518_UQ.dat\n",
    "39412 hdfs://node1.fabric.zones.eait.uq.edu.au:8020/shared/silo/56017_UQ.dat\n",
    "39345 hdfs://node1.fabric.zones.eait.uq.edu.au:8020/shared/silo/9534_UQ.dat\n",
    "```\n",
    "\n",
    "There's one new column you may not be familiar with which is Hive specific - `INPUT__FILE__NAME`. Since Hive is just a front-end for performing work on actual files in HDFS, you can use the virtual column name `INPUT__FILE__NAME` to pull the actual file name the data is sourced from. \n",
    "\n",
    "In our case, we performed a count of every match where smx = 0, and grouped the results by the filename. **Also note the `LIMIT 10` at the end**. Adding a LIMIT is important when working with files this size - if you print out hundreds and thousands of lines of data you will be overloaded with output, it's best to limit the results so they are easier to manage and understand. This, along with `ORDER BY` which makes sure it prints in descending order of the result of `count(*)`.\n",
    "\n",
    "From these results, we can see that the `66062_UQ.dat` file has the highest frequency of Station Data. We can query the 'mdata' table to find out more about this dataset:\n",
    "\n",
    "```\n",
    "hive> select * from mdata where fname='66062_UQ.dat';\n",
    "OK\n",
    "66062_UQ.dat    -33.8607        39.0    66062   151.205 SYDNEY (OBSERVATORY HILL)\n",
    "Time taken: 0.697 seconds, Fetched: 1 row(s)\n",
    "```\n",
    "\n",
    "So it looks like this dataset is located in Sydney.\n",
    "\n",
    "What if we want to find all the datasets that are close to our location of interest in `SYDNEY (OBSERVATORY HILL)`? As all the location data is contained within a small, single metadata file, let's go back to Python for a moment to see if we can find out which observatories are closest to our data point of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from vincenty import vincenty\n",
    "\n",
    "metadata = pd.read_csv(\"all_metadata.csv\")\n",
    "\n",
    "to_sort = []\n",
    "base_coords = (-33.8607, 151.205) #co-ordinates for id: 66062\n",
    "for i in range(0, len(metadata)):\n",
    "    to_sort.append(\n",
    "        (metadata['id'][i],\n",
    "         vincenty(base_coords, (metadata['latitude'][i], metadata['longitude'][i]),miles=False) )\n",
    "    )\n",
    "    \n",
    "to_sort.sort(key=lambda tup: tup[1])\n",
    "to_sort[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code reads from the `all_metadata` file and uses the Vincenty's Forumlae (https://en.wikipedia.org/wiki/Vincenty%27s_formulae) to determine the distance between our original point of interest (-33.8607, 151.205) and each other position contained within our dataset. We then list the first 20 sorted elements by distance in kilometers to find our closest datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
