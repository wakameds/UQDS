{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SILO Patch Point Dataset\n",
    "\n",
    "SILO is an enhanced climate database hosted by the Science Delivery Division of the Department of Science, Information Technology and Innovation (DSITI). SILO contains Australian climate data from 1889 (current to yesterday), suitable for research and climate applications. More information is available at the [SILO Climate Data](https://www.longpaddock.qld.gov.au/silo/) website.\n",
    "\n",
    "All SILO climate data has been provided to UQ, and has been downloaded and is available on the Master of Data Science Datasets [website](https://mdatascience.uqcloud.net/SILO_PPD/) in the form of space-deliminated files with a daily time series of data at point locations consisting of station records which have been supplemented by interpolated estimates when observed data are missing. Patched datasets are available at approximately 4800 Bureau of Meteorology recording stations around Australia.\n",
    "\n",
    "**The datasets provided are only to be used for learning purposes as specified in the respective course(s). These datasets are available publicly (unless otherwise stated), use of these publicly available datasets is strictly limited to learning purposes in the master of data science course. If you wish to use these datasets for any other purpose please access them directly from the source. UQ accepts no responsibility for any use of these datasets outside of these conditions.**\n",
    "\n",
    "## Proposed data-science problems\n",
    "\n",
    "SILO has suggested a number of problems that might be worth looking into with this dataset:\n",
    "\n",
    "### Error detection in climate data\n",
    "\n",
    "The project would aim to develop an error detection system for identifying erroneous values in climate datasets. The proposed system could use a range of algorithms, from simple range checking (to ensure data values are within an acceptable range), to advanced machine learning or dynamic climate modelling methods. The analysis could examine previous observations from the same station records (temporal analysis) and/or station records from nearby stations (spatial analysis).  Other sources of data verification could be used if they are readily available (for example, weather forecasts, satellite estimates, etc.). Students are encouraged to propose any viable method even if it doesn’t result in usable software. Participating students are expected to outlinethe proposed methodology and plan for evaluation.\n",
    "\n",
    "### Detecting trends in climate datasets\n",
    "\n",
    "The project would aims to identify systematic trends or biases in the “Patched Point Datasets (PPDs)” provided by SILO. A “patched” dataset is a daily time series of climate data at a given location, consisting of observed data for a given variable (for example, rainfall) when observed data are available, and interpolated data when observed data are missing. The goal is to develop an algorithm which can determine if there is a significant difference between observed and in-filled (interpolated) data. The determination should be made by computing statistical parameters for period(s) when observed data are provided, and for period(s) when interpolated estimates are provided.  Ambitious students may also wish to investigate methods for adjusting the interpolated data so it has statistical properties (eg. mean, variance) matching the observed data.\n",
    "\n",
    "### Predicting future values and/or missing stations\n",
    "\n",
    "Can you use this dataset to either predict future values at an existing station, predict the values of a missing station at a particularly geospation location or predict the value of a particular weather observation in time from other observations (such as temperature from insolation). Either of these approaches could be investigated by keeping appropriate data separate, building models, and comparing the output of those models to the left-out data.\n",
    "\n",
    "## Getting The Data I Need\n",
    "\n",
    "The SILO data is provided as around 4800 space-deliminated files, with the list of files provided in the file [all_files.txt](https://stluc.manta.uqcloud.net/mdatascience/public/datasets/SILO_PPD/all_files.txt) You can also browse the files manually using the data science browser at: https://mdatascience.uqcloud.net/SILO_PPD/\n",
    "\n",
    "As the files account to almost 200GB, they have also been placed into a shared HDFS directory under `/shared/silo` and imported into Hive for further exploration.\n",
    "\n",
    "\n",
    "## Is my data fit for use?\n",
    "The first thing you may notice is that there are many individual files, totalling about 200GB. This amount of data is far larger than what can fit into memory on a single machine. However, the individual files alone are small enough to fit in memory and analyse using traditional tools. One of the largest challenge of big data problems is deciding when using a cluster might be appropriate and when using our traditional scripting tools such as Python or R might be.\n",
    "\n",
    "In our case, we can use Hive/distributed processing to ask questions about the *entire dataset*, which may guide us into what files are worth looking at for closer inspection. Since submitting a job over a large dataset can take a lot of time, it's worth examining individual files first to determine what sort of questions you want answered about the entire dataset.\n",
    "\n",
    "Let's first see how many files there are, we will use Python to scrape some information from the data repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from IPython.display import display\n",
    "r = requests.get('https://stluc.manta.uqcloud.net/mdatascience/public/datasets/SILO_PPD/all_files.txt')\n",
    "all_files = [f.strip() for f in r.text.split('\\n') if f.strip()]\n",
    "print(\"There are {} PPD files available\".format(len(all_files)))\n",
    "print(\"First 10 files:\")\n",
    "display(all_files[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting file metadata\n",
    "Each file has a very large header in the CSV before it gets down to the actual data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_lines(filename):\n",
    "    r = requests.get('https://stluc.manta.uqcloud.net/mdatascience/public/datasets/SILO_PPD/{}'\n",
    "                   .format(filename))\n",
    "    return [l.strip() for l in r.text.split('\\n')]\n",
    "print '\\n'.join(get_file_lines('3006_UQ.dat')[0:55])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This header is identical for all files, with the exception of the lines\n",
    "\n",
    "```\" * Patched Point data for station:  3006 FITZROY CROSSING COMP.                   Lat: -18.1919 Long: 125.5644\"\n",
    "\" * Elevation:  114m \"```\n",
    "\n",
    "So, let's see if we can get that extracted for all files (and save it for use later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def get_metadata(filename):\n",
    "    metadata={}\n",
    "    r = requests.get('https://stluc.manta.uqcloud.net/mdatascience/public/datasets/SILO_PPD/{}'\n",
    "                   .format(filename))\n",
    "    for line in r.text.split('\\n'):\n",
    "        # Use regular expressions to pattern match\n",
    "        # Effectively, we want to match the string after \"station:\" and the floats after\n",
    "        # \"Lat: \" and \"Long: \"\n",
    "        PPD_match = re.search('station: *(?P<id>[^ ]+)' +\n",
    "                              ' (?P<name>.*) ' +\n",
    "                              'Lat: (?P<latitude>[0-9.-]+) ' +\n",
    "                              'Long: (?P<longitude>[0-9/.-]+)', line)\n",
    "        if PPD_match:\n",
    "            metadata['id'] = PPD_match.group('id')\n",
    "            metadata['name'] = PPD_match.group('name').strip()\n",
    "            metadata['latitude'] = float(PPD_match.group('latitude'))\n",
    "            metadata['longitude'] = float(PPD_match.group('longitude'))\n",
    "        else:\n",
    "            elevation_match = re.search('Elevation: *(?P<elevation>[0-9]+)m', line)\n",
    "            if elevation_match:\n",
    "                metadata['elevation'] = int(elevation_match.group('elevation'))\n",
    "                # once we have the elevation, all metadata is collected, and we can stop\n",
    "                break\n",
    "            \n",
    "    return metadata\n",
    "            \n",
    "get_metadata('3006_UQ.dat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like it worked. Let's try it on all files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "all_metadata = pd.read_csv('all_metadata.csv')\n",
    "for i in range(all_metadata.shape[0]):\n",
    "    if pd.isnull(all_metadata.loc[i,'name']):\n",
    "        filename = all_metadata.loc[i,'filename']\n",
    "        print(\"Extracting metadata from {} ({}/{})\".format(filename, i+1, len(all_files)))\n",
    "        for k, v in get_metadata(filename).items():\n",
    "            all_metadata.loc[i,k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_metadata.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've extracted the metadata from the files, let's save this to disk as a new csv called `all_metadata.csv` we can use for future analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metadata.to_csv('all_metadata.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata\n",
    "\n",
    "Now that we have parsed the metadata out of the raw data, let's use it to visualise some of the data. Since we have latitutde and longitude, we can the location of each data point on a map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(all_metadata['longitude'],all_metadata['latitude'],'r,')\n",
    "plt.title(\"Weather station locations\")\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like most of the weather station locations are centred around major city areas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weather-station data\n",
    "\n",
    "Let's choose a station at random, and see what sort of data we can retrieve. According to the documentation we should get the following fields:\n",
    "\n",
    "|Field|Type|Description|\n",
    "| ---- | \n",
    "| date | string (yyyymmdd) | date of observation |\n",
    "| day | integer | day of year |\n",
    "| date2 | string (dd-mm-yyyy) | date of observation (just different format) |\n",
    "| T.Max | float | maximum temperature reached (Celsius) |  \n",
    "| Smx | integer | source of maximum temperature (see Sources) |\n",
    "| T.Min | float | minimum temperature reached (Celsius) |\n",
    "| Smn | integer | source of minimum temperature (see Sources) |\n",
    "| Rain | float | millimetres of rainfall |\n",
    "| Srn | integer | source of rainfall (see Sources) |\n",
    "| Evap | float | millimetres of evaporation | \n",
    "| Sev | integer | source of evaporation (see Sources) |\n",
    "| Radn | float | radiation (in MJ/m^2) |\n",
    "| Ssl | integer | source of radiation (see Sources) |\n",
    "| VP | float | vapour pressure (in hPa) |\n",
    "| Svp | integer | source of vapour pressure |\n",
    "| RHmaxT | float | estimated relative humidity at T.Max (percent 0-100) |\n",
    "| RHminT | float | estimated relative humidity at T.Min (percent 0-100) |\n",
    "| FAO56 | float | [Short-crop potential evapotranspiration](http://www.fao.org/docrep/X0490E/X0490E00.htm) (mm) |\n",
    "| ASCEPM | float | [ASCE tall-crop potential evapotranspiration](http://www.kimberly.uidaho.edu/water/asceewri/ASCE_Standardized_Ref_ET_Eqn_Phoenix2000.pdf) (mm) |\n",
    "| Mlake | float | Morton evaporation over shallow lakes (mm) |\n",
    "| Mpot | float | Morton potential evapotranspiration over land (mm) |\n",
    "| Mact | float | Morton actual evapotranspiration over land (mm) |\n",
    "| Mwet | float | Morton wet environment areal evapotranspiration over land (mm) |\n",
    "| Span | float | calibrated estimate of class A pan evaporation (mm) |\n",
    "| Ssp | integer | source of pan evaporation (see Sources) |\n",
    "| EvSp | float | class A evaporation (used post 1970) and synthetic pan evaporation (pre 1970) (mm) |\n",
    "| MSLPres | float | mean sea level pressure (hPa) |\n",
    "| Sp | integer | source of MSLPres (see Sources) |\n",
    "\n",
    "#### Sources\n",
    "\n",
    "Many of the above fields have a corresponding 'source' field, which is an integer code corresponding to the following sources:\n",
    "\n",
    "| source code | description\n",
    "| ---------- |\n",
    "| 0 | Station data, as supplied by Bureau |\n",
    "| 13 | Deaccumulated using nearby station |\n",
    "| 15 | Deaccumulated using interpolated data |\n",
    "| 23 | Nearby station, data from BoM | \n",
    "| 25 | interpolated daily observations |\n",
    "| 26 | synthetic pan evaporation (only used for Ssp) |\n",
    "| 35 | interpolated from daily observations using anomaly interpolation method for CLIMARC data | \n",
    "| 75 | interpolated long term average | \n",
    "\n",
    "Let's put those into a variable, for use later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_names = {0: 'station',\n",
    "               13: 'deaccum-nearby',\n",
    "               15: 'deaccum-interp',\n",
    "               23: 'nearby-BoM',\n",
    "               25: 'interp-daily',\n",
    "               26: 'synth-pan',\n",
    "               35: 'interp-daily-CLIMARC',\n",
    "               75: 'interp-lta'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load up a station, and have a look at its data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import random\n",
    "\n",
    "def choose_station():\n",
    "    choice_index = random.randint(0,len(all_metadata)-1)\n",
    "    return all_metadata.iloc[choice_index,:]\n",
    "\n",
    "choice = all_metadata.iloc[1,:]\n",
    "# choice = choose_station()\n",
    "print choice\n",
    "\n",
    "# basemap\n",
    "plt.figure(figsize=[10,10])\n",
    "plt.plot(all_metadata['longitude'],all_metadata['latitude'],'r,')\n",
    "plt.title(\"Weather station locations\")\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "# chosen weather station\n",
    "x, y = [choice['longitude'], choice['latitude']]\n",
    "plt.plot(x, y, 'b.')\n",
    "plt.annotate('{} [{}]'.format(choice['name'], choice['filename']), xy=[x, y], xytext=[x, y+0.3], color='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, let's grab that file, and see what we've got:\n",
    "\n",
    "We learnt from the metadata extraction script that the first 53 lines are various documentation and can be ignored. line 54 is the headers, but line 55 is units and can be ignore, at least for data input. We should be able to use `header=53` (zero-indexed) and `skiprows=[54]` to do this, but something is weird in Pandas, and actually, `header=47` and `skiprows=[48]` appears to work, so we'll move on for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(choice):\n",
    "    url = 'https://stluc.manta.uqcloud.net/mdatascience/public/datasets/SILO_PPD/' + choice['filename']\n",
    "    print (\"Loading {}\".format(url))\n",
    "    data = pd.read_csv(url, header=47, skiprows=[48], skip_blank_lines=False, \n",
    "                       delim_whitespace=True, parse_dates=[2], dayfirst=True)\n",
    "    return data\n",
    "\n",
    "data = load_data(choice)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the temperatures over time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[15,5])\n",
    "plt.plot(data['Date2'],data['T.Max'],'r-')\n",
    "plt.plot(data['Date2'],data['T.Min'],'g-')\n",
    "plt.legend(['T.Max','T.min'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's smooth the data and see if we can get a clearer long term picture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we should smooth that and see if we can get a clearer long-term picture\n",
    "TMax_smoothed = data['T.Max'].rolling(window=30).mean()\n",
    "TMin_smoothed = data['T.Min'].rolling(window=30).mean()\n",
    "plt.figure(figsize=[15,5])\n",
    "plt.plot(data['Date2'],data['T.Max'],'r-', alpha=0.1)\n",
    "plt.plot(data['Date2'],TMax_smoothed,'r-')\n",
    "plt.plot(data['Date2'],data['T.Min'],'g-', alpha=0.1)\n",
    "plt.plot(data['Date2'],TMin_smoothed,'g-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this specific station, it doesn't look like there are any obvious patterns of temperature changes over time, except from around 1961 where the pattern seems to change. Why is this? You may recall from earlier that the data contains a variety of different *sources*. Some data is measured, and some is interpolated. Part of determining if our data is fit for use is to make sure that the data is accurate, which may involve determining examining the sources. Let's see where this data is sourced from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at the sources of TMax and TMin for this station\n",
    "import numpy as np\n",
    "indexes = np.arange(0, len(source_names))\n",
    "fig, ax = plt.subplots(figsize=(15,5))\n",
    "width=0.35\n",
    "h1 = ax.bar(indexes,\n",
    "        [sum(data['Smx'] == k) for k in source_names.keys()],\n",
    "      color='r', width=width)\n",
    "h2 = ax.bar(indexes + width,\n",
    "        [sum(data['Smn'] == k) for k in source_names.keys()],\n",
    "      color='g', width=width)\n",
    "ax.set_xticks(indexes + width/2)\n",
    "ax.set_xticklabels(source_names.values());\n",
    "ax.legend((h1[0], h2[0]), ('T.Max', 'T.Min'))\n",
    "ax.set_title('Source of Temperature Data for {}'.format(choice['name']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like much of the data is interpolated, rather than measured, using two different methods of interpolation. Maybe we can look at these sources over time to determine when each method of interpolation was used?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# create a function, so we can call it multiple times\n",
    "def plot_data_by_source(data, val_field, source_field, name, date_field='Date2', source_names=source_names):\n",
    "\n",
    "    # helper function to ensure we stay within the size of colors\n",
    "    def get_color(i):\n",
    "        colors = plt.get_cmap('tab10').colors\n",
    "        return colors[i % len(colors)]\n",
    "\n",
    "    plt.figure(figsize=[15,5])\n",
    "\n",
    "    handles = []\n",
    "\n",
    "    for i, s in enumerate(source_names.keys()):\n",
    "        \n",
    "        select = (data[source_field] == s)\n",
    "        if sum(select) > 0:\n",
    "            plt.plot(data[date_field][select],data[val_field][select],'.', color=get_color(i))\n",
    "            # create handle for legend\n",
    "            handles.append(mpatches.Patch(color=get_color(i), \n",
    "                                          label='{} ({:0.2f}%)'.format(\n",
    "                                              source_names[s], \n",
    "                                              100.0*sum(select)/len(select))))\n",
    "    \n",
    "    # legend\n",
    "    plt.legend(handles=handles)\n",
    "    plt.title('Source of {} data for {}'.format(val_field, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_by_source(data, 'T.Max', 'Smx', '{} [{}]'.format(choice['name'], choice['filename']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like in 1961 the source changed, and actual data was used rather than interpolated data. This explains the shift in temperatures we saw during that time period.\n",
    "\n",
    "Let's pick another *random* station and see if the source of the data also uses the same methods of interpolations. Make sure you run the following code (with shift+enter on the block):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# now lets look at another\n",
    "new_choice = choose_station()\n",
    "plot_data_by_source(load_data(new_choice), 'T.Max', 'Smx', \n",
    "                    '{} [{}]'.format(new_choice['name'], new_choice['filename']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does this differ? Are the sources of this data different? Remember, since the dataset you picked is *random*, each student will have different silo data. Try running the following blocks of code again to continue to pick random locations, and get a feel of how the sources change depending on location. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now lets look at another\n",
    "new_choice = choose_station()\n",
    "plot_data_by_source(load_data(new_choice), 'T.Max', 'Smx', \n",
    "                    '{} [{}]'.format(new_choice['name'], new_choice['filename']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The difference between sources suggests that some datasets might be more trustworthy than others. When determining if our data is fit for use, or what dataset to focus on, we will have to also take into account *how* that data was obtained, as it will change how we should analyse it.\n",
    "\n",
    "Given that there are 4000 files, this technique of looking at each one manually to find trustworthy datasets to further analyse on a single computer would be time consuming. In the next session, we will use our hadoop cluster to perform distributed analysis to get an understanding of the data *as a whole* rather than individual files, and use that knowledge to select a few representative files to analyse further."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
